# Native Language Identification

The aim of this project is to identify someone's native language based on their usage of a later-learned language (in this case English). We distinguish eleven different native languages, using the data as used in the ComParE 2016 NLI sub-challenge. In addition to the 45-second audio segments, we create linguistic data by passing the audio to an ASR speech-to-text tool, increasing the raw data that is available. We use both traditional acoustic features such as MFCC, RASTA-PLP, and acoustic functionals, as well as state-of-the-art contextual acoustic embeddings created using Wav2vec 2.0. We extract acoustic word embeddings using BERT (and derivatives). Both types of low-level features are then passed to an utterance-level encoding mechansim (such as Fisher Vector). We achieve incredibly fast classification using Extreme Learning Machines with linear Kernels. Fusion of the linguistic modalities happens both at feature-level, as well as score level.

The necessary packages for each file are listed at the top. The transformer-based acoustic and linguistic embeddings are extracted using the Huggingface Transformers library. As a prerequisite you need to have both the audio files, the transcripts generated using Google Cloud ASR tts engine, and the MFCC+RASTA-PLPC features extracted using Matlab. Next the, scripts in the data_creation folder can be run individually to generate the different types of featuress. Fisher Vector encodings can be generated for all low-level descriptors file types. Main.py contains the methods necessary for hyperparameter optimization.